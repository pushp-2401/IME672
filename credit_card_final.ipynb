{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "credit_card_final",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMtSSUPnGIDUOrWPV/jxUIu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushp-2401/IME672/blob/main/credit_card_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcGFOuBT8HUX"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIihMGxu8Lec"
      },
      "source": [
        "df = pd.read_csv('creditcard.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqAksZK48N8b"
      },
      "source": [
        "df = df[df.Amount < 10000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TDpiyy_8Rkw"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "\n",
        "rob_scaler = RobustScaler()\n",
        "\n",
        "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
        "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
        "\n",
        "df.drop(['Time','Amount'], axis=1, inplace=True)\n",
        "df = df[['scaled_time','scaled_amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
        "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
        "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\n",
        "       'Class']]\n",
        "print('Scaled Data\\n')\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmpuwaU_8Udw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
        "\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "    # print(\"Train:\", train_index, \"Test:\", test_index)\n",
        "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
        "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "print(\"df shape\", df.shape)\n",
        "print(\"original_Xtrain shape\", original_Xtrain.shape)\n",
        "print(\"original_ytrain shape\", original_ytrain.shape)\n",
        "print(\"original_Xtest shape\", original_Xtest.shape)\n",
        "print(\"original_ytest shape\", original_ytest.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3yG4FIC8XqR"
      },
      "source": [
        "credit_card_4_5 = pd.concat([pd.DataFrame(original_Xtrain), pd.DataFrame(original_ytrain)], axis=1)\n",
        "credit_card_1_5 = pd.concat([pd.DataFrame(original_Xtest), pd.DataFrame(original_ytest)], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s794i2hX8bTN"
      },
      "source": [
        "credit_card_1_5.to_csv(\"credit_card_1_5.csv\")\n",
        "credit_card_4_5.to_csv(\"credit_card_4_5.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkAUnARp8eI6"
      },
      "source": [
        "df = credit_card_4_5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X-t22o78hdV"
      },
      "source": [
        "nonfraud = df[df.Class == 0]\n",
        "fraud = df[df.Class == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTYov9ZA8kZS"
      },
      "source": [
        "print(fraud.shape)\n",
        "print(nonfraud.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZd-1nR68nTq"
      },
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-TTexIm8sE6"
      },
      "source": [
        "X_test = credit_card_1_5.drop('Class',axis=1)\n",
        "y_test = credit_card_1_5['Class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTVODmaP8ua0"
      },
      "source": [
        "nonfraud_sample = nonfraud.sample(n=1000)\n",
        "df_outlier=pd.concat([fraud,nonfraud_sample])\n",
        "X_train = df_outlier.drop('Class', axis=1)\n",
        "y_train = df_outlier['Class']\n",
        "\n",
        "state = 1\n",
        "outlier_fraction = len(fraud)/float(len(nonfraud_sample))\n",
        "\n",
        "clf = IsolationForest(max_samples=len(X_train), contamination = outlier_fraction, random_state = state)\n",
        "clf.fit(X_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "y_pred[y_pred == 1] = 0\n",
        "y_pred[y_pred == -1] = 1\n",
        "y_pred = pd.DataFrame(y_pred)\n",
        "\n",
        "mat = confusion_matrix(y_test,y_pred)\n",
        "print(mat)\n",
        "print(accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Total number of Transactions classified as Fraudulent: \", mat[1][1]+mat[0][1])\n",
        "print(\"Number of Fraudulent Transactions classified as Non-fraudulent: \", mat[1][0], \"out of 98\")\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from matplotlib import pyplot\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(fpr, tpr, marker='.')\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "pyplot.title('ROC curve when n = 1000')\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-4ugjPc8xgA"
      },
      "source": [
        "nonfraud_sample = nonfraud.sample(n=45000)\n",
        "df_outlier=pd.concat([fraud,nonfraud_sample])\n",
        "X_train = df_outlier.drop('Class', axis=1)\n",
        "y_train = df_outlier['Class']\n",
        "\n",
        "state = 1\n",
        "outlier_fraction = len(fraud)/float(len(nonfraud_sample))\n",
        "\n",
        "clf = IsolationForest(max_samples=len(X_train), contamination = outlier_fraction, random_state = state)\n",
        "clf.fit(X_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "y_pred[y_pred == 1] = 0\n",
        "y_pred[y_pred == -1] = 1\n",
        "y_pred = pd.DataFrame(y_pred)\n",
        "\n",
        "mat = confusion_matrix(y_test,y_pred)\n",
        "print(mat)\n",
        "print(accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Total number of Transactions classified as Fraudulent: \", mat[1][1]+mat[0][1])\n",
        "print(\"Number of Fraudulent Transactions classified as Non-fraudulent: \", mat[1][0], \"out of 98\")\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from matplotlib import pyplot\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(fpr, tpr, marker='.')\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "pyplot.title('ROC curve when n = 1000')\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnsmkxGD80nS"
      },
      "source": [
        "nonfraud_sample = nonfraud.sample(n=5000)\n",
        "df_outlier=pd.concat([fraud,nonfraud_sample])\n",
        "X_train = df_outlier.drop('Class', axis=1)\n",
        "y_train = df_outlier['Class']\n",
        "\n",
        "state = 1\n",
        "outlier_fraction = len(fraud)/float(len(nonfraud_sample))\n",
        "\n",
        "clf = IsolationForest(max_samples=len(X_train), contamination = outlier_fraction, random_state = state)\n",
        "clf.fit(X_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "y_pred[y_pred == 1] = 0\n",
        "y_pred[y_pred == -1] = 1\n",
        "y_pred = pd.DataFrame(y_pred)\n",
        "\n",
        "mat = confusion_matrix(y_test,y_pred)\n",
        "print(mat)\n",
        "print(\"Accuracy for Isolation Forest: \", accuracy_score(y_test, y_pred)*100)\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Total number of Transactions classified as Fraudulent: \", mat[1][1]+mat[0][1])\n",
        "print(\"Number of Fraudulent Transactions classified as Non-fraudulent: \", mat[1][0], \"out of 98\")\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from matplotlib import pyplot\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(fpr, tpr, marker='.')\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "pyplot.title('ROC curve when n = 1000')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl7M8iT187l2"
      },
      "source": [
        "import seaborn as sn\n",
        "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), index = [i for i in ['Non-Fraudulent','Fraudulent']],\n",
        "                  columns = [i for i in ['Non-Fraudulent','Fraudulent']])\n",
        "plt.figure(figsize = (10,7))\n",
        "plt.title(\"Gaussian Anomaly Detection\")\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNpxuNTw8-_C"
      },
      "source": [
        "y_pred_iso = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDm2DUhf9Bnk"
      },
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "def estimateGaussian(dataset):\n",
        "    mu = np.mean(dataset, axis=0)\n",
        "    sigma = np.cov(dataset.T)\n",
        "    return mu, sigma\n",
        "\n",
        "def multivariateGaussian(dataset, mu, sigma):\n",
        "    p = multivariate_normal(mean = mu, cov = sigma)\n",
        "    return p.pdf(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUYcgGUL9EE4"
      },
      "source": [
        "\n",
        "def selectThresholdByCV(probs,gt):\n",
        "    best_epsilon = 0\n",
        "    best_f1 = 0\n",
        "    f = 0\n",
        "    farray = []\n",
        "    Recallarray = []\n",
        "    Precisionarray = []\n",
        "    epsilons = (0.0000e+00, 1.0527717316e-70, 1.0527717316e-50, 1.0527717316e-24)\n",
        "    for epsilon in epsilons:\n",
        "        predictions = (p_cv < epsilon)\n",
        "        f = f1_score(train_cv_y, predictions, average = \"binary\")\n",
        "        Recall = recall_score(train_cv_y, predictions, average = \"binary\")\n",
        "        Precision = precision_score(train_cv_y, predictions, average = \"binary\")\n",
        "        farray.append(f)\n",
        "        Recallarray.append(Recall)\n",
        "        Precisionarray.append(Precision)\n",
        "        print ('For below Epsilon')\n",
        "        print(epsilon)\n",
        "        print ('F1 score , Recall and Precision are as below')\n",
        "        print ('Best F1 Score %f' %f)\n",
        "        print ('Best Recall Score %f' %Recall)\n",
        "        print ('Best Precision Score %f' %Precision)\n",
        "        print ('-'*40)\n",
        "        if f > best_f1:\n",
        "            best_f1 = f\n",
        "            best_recall = Recall\n",
        "            best_precision = Precision\n",
        "            best_epsilon = epsilon    \n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_axes([0.1, 0.5, 0.7, 0.3])\n",
        "    #plt.subplot(3,1,1)\n",
        "    plt.plot(farray ,\"ro\")\n",
        "    plt.plot(farray)\n",
        "    ax.set_xticks(range(5))\n",
        "    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n",
        "    ax.set_ylim((0,1.0))\n",
        "    ax.set_title('F1 score vs Epsilon value')\n",
        "    ax.annotate('Best F1 Score', xy=(best_epsilon,best_f1), xytext=(best_epsilon,best_f1))\n",
        "    plt.xlabel(\"Epsilon value\") \n",
        "    plt.ylabel(\"F1 Score\") \n",
        "    plt.show()\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_axes([0.1, 0.5, 0.9, 0.3])\n",
        "    #plt.subplot(3,1,2)\n",
        "    plt.plot(Recallarray ,\"ro\")\n",
        "    plt.plot(Recallarray)\n",
        "    ax.set_xticks(range(5))\n",
        "    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n",
        "    ax.set_ylim((0,1.0))\n",
        "    ax.set_title('Recall vs Epsilon value')\n",
        "    ax.annotate('Best Recall Score', xy=(best_epsilon,best_recall), xytext=(best_epsilon,best_recall))\n",
        "    plt.xlabel(\"Epsilon value\") \n",
        "    plt.ylabel(\"Recall Score\") \n",
        "    plt.show()\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_axes([0.1, 0.5, 0.9, 0.3])\n",
        "    #plt.subplot(3,1,3)\n",
        "    plt.plot(Precisionarray ,\"ro\")\n",
        "    plt.plot(Precisionarray)\n",
        "    ax.set_xticks(range(5))\n",
        "    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n",
        "    ax.set_ylim((0,1.0))\n",
        "    ax.set_title('Precision vs Epsilon value')\n",
        "    ax.annotate('Best Precision Score', xy=(best_epsilon,best_precision), xytext=(best_epsilon,best_precision))\n",
        "    plt.xlabel(\"Epsilon value\") \n",
        "    plt.ylabel(\"Precision Score\") \n",
        "    plt.show()\n",
        "    return best_f1, best_epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCpjRGJy9H0I"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Feature Importance\n",
        "# To ease the Problem at hand, we use Feature importance to get rid \n",
        "# of unwanted features whose existance will not improve our prediction model. \n",
        "\n",
        "# For that, I have used random forest classifier to identify the influential fetures. \n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
        "rnd_clf.fit(df.iloc[:,2:29],df.iloc[:,30]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMXpx1Uh9K2S"
      },
      "source": [
        "# Visualizing feature importance\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "x, y = (list(x) for x in zip(*sorted(zip(rnd_clf.feature_importances_, df.iloc[:,2:29].columns), \n",
        "                                     reverse = False)))\n",
        "trace2 = go.Bar(\n",
        "    x = x ,\n",
        "    y = y,\n",
        "    marker = dict(color=x, colorscale = 'Viridis', reversescale = True),\n",
        "    name = 'Random Forest Feature importance',\n",
        "    orientation = 'h',\n",
        ")\n",
        "\n",
        "layout = dict(\n",
        "    title='Barplot of Feature importances',\n",
        "    width = 600, height = 1000,\n",
        "    yaxis=dict( showgrid=False, showline=False, showticklabels=True),\n",
        "    margin=dict(l=300),\n",
        ")\n",
        "\n",
        "fig1 = go.Figure(data=[trace2], layout=layout)\n",
        "iplot(fig1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPue7he99OHY"
      },
      "source": [
        "im = plt.imread('feature_importance.png')\n",
        "# show the image\n",
        "plt.imshow(im)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYFq7lq09QyH"
      },
      "source": [
        "print(\"Relatively More Important Features\")\n",
        "for name, importance in zip(df.iloc[:,2:29].columns, rnd_clf.feature_importances_):\n",
        "    if importance > 0.015 :\n",
        "        print('\"' + name + '\"'+',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8MBJhjJ9UNs"
      },
      "source": [
        "drop_features = ['V1','V2','V5','V6','V8','V13','V15','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28']\n",
        "df.drop(drop_features, axis =1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Kj0jcf9Wu7"
      },
      "source": [
        "train_strip_v1 = df[df[\"Class\"] == 1]\n",
        "train_strip_v0 = df[df[\"Class\"] == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6A1ek5O9Zff"
      },
      "source": [
        "Normal_len = len (train_strip_v0)\n",
        "Anomolous_len = len (train_strip_v1)\n",
        "\n",
        "start_mid = Anomolous_len // 2\n",
        "start_midway = start_mid + 1\n",
        "\n",
        "train_cv_v1  = train_strip_v1 [: start_mid]\n",
        "train_test_v1 = train_strip_v1 [start_midway:Anomolous_len]\n",
        "\n",
        "start_mid = (Normal_len * 60) // 100\n",
        "start_midway = start_mid + 1\n",
        "\n",
        "cv_mid = (Normal_len * 80) // 100\n",
        "cv_midway = cv_mid + 1\n",
        "\n",
        "train_fraud = train_strip_v0 [:start_mid]\n",
        "train_cv    = train_strip_v0 [start_midway:cv_mid]\n",
        "train_test  = train_strip_v0 [cv_midway:Normal_len]\n",
        "\n",
        "train_cv = pd.concat([train_cv,train_cv_v1],axis=0)\n",
        "train_test = pd.concat([train_test,train_test_v1],axis=0)\n",
        "\n",
        "train_cv_y = train_cv[\"Class\"]\n",
        "train_test_y = train_test[\"Class\"]\n",
        "\n",
        "train_cv.drop(labels = [\"Class\"], axis = 1, inplace = True)\n",
        "train_fraud.drop(labels = [\"Class\"], axis = 1, inplace = True)\n",
        "train_test.drop(labels = [\"Class\"], axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BouJRKEX9dVc"
      },
      "source": [
        "mu, sigma = estimateGaussian(train_fraud)\n",
        "p = multivariateGaussian(train_fraud,mu,sigma)\n",
        "p_cv = multivariateGaussian(train_cv,mu,sigma)\n",
        "p_test = multivariateGaussian(train_test,mu,sigma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moWmn9dO9gEt"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score , average_precision_score\n",
        "from sklearn.metrics import precision_score, precision_recall_curve\n",
        "\n",
        "fscore, ep= selectThresholdByCV(p_cv,train_cv_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0THMTKAw9ixR"
      },
      "source": [
        "print(\"PREDICTIONS ON CROSS-VALIDATION SET\\n\")\n",
        "predictions = (p_cv < ep)\n",
        "Recall = recall_score(train_cv_y, predictions, average = \"binary\")    \n",
        "Precision = precision_score(train_cv_y, predictions, average = \"binary\")\n",
        "F1score = f1_score(train_cv_y, predictions, average = \"binary\")    \n",
        "print ('F1 score , Recall and Precision for Cross Validation dataset')\n",
        "print ('Best F1 Score %f' %F1score)\n",
        "print ('Best Recall Score %f' %Recall)\n",
        "print ('Best Precision Score %f' %Precision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSNYV0_79mPq"
      },
      "source": [
        "print(\"PREDICTIONS ON TEST SET\\n\")\n",
        "predictions = (p_test < ep)\n",
        "Recall = recall_score(train_test_y, predictions, average = \"binary\")    \n",
        "Precision = precision_score(train_test_y, predictions, average = \"binary\")\n",
        "F1score = f1_score(train_test_y, predictions, average = \"binary\")    \n",
        "print ('F1 score , Recall and Precision for Test dataset')\n",
        "print ('Best F1 Score %f' %F1score)\n",
        "print ('Best Recall Score %f' %Recall)\n",
        "print ('Best Precision Score %f' %Precision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GexdBDY99rfb"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.scatter(train_test['V14'],train_test['V11'],marker=\"o\", color=\"lightBlue\")\n",
        "ax.set_title('Anomalies(in red) vs Predicted Anomalies(in Green)')\n",
        "for i, txt in enumerate(train_test['V14'].index):\n",
        "       if train_test_y.loc[txt] == 1 :\n",
        "            ax.annotate('*', (train_test['V14'].loc[txt],train_test['V11'].loc[txt]),fontsize=13,color='Red')\n",
        "       if predictions[i] == True :\n",
        "            ax.annotate('o', (train_test['V14'].loc[txt],train_test['V11'].loc[txt]),fontsize=15,color='Green')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BARL1Esr9sBS"
      },
      "source": [
        "test_df = credit_card_1_5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqxhbEMo9u7w"
      },
      "source": [
        "drop_features = ['V1','V2','V5','V6','V8','V13','V15','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28']\n",
        "test_df.drop(drop_features, axis =1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPrhxaOj9ycS"
      },
      "source": [
        "test_y = test_df['Class']\n",
        "\n",
        "test_df.drop(labels = [\"Class\"], axis = 1, inplace = True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTRMfzn894JH"
      },
      "source": [
        "p_test_df = multivariateGaussian(test_df,mu,sigma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOyoWdSn96mh"
      },
      "source": [
        "print(\"PREDICTIONS ON ORIGINAL TEST SET\\n\")\n",
        "predictions = (p_test_df < ep)\n",
        "Recall = recall_score(test_y, predictions, average = \"binary\")    \n",
        "Precision = precision_score(test_y, predictions, average = \"binary\")\n",
        "F1score = f1_score(test_y, predictions, average = \"binary\")    \n",
        "print ('F1 score , Recall and Precision for Test dataset')\n",
        "print ('Best F1 Score %f' %F1score)\n",
        "print ('Best Recall Score %f' %Recall)\n",
        "print ('Best Precision Score %f' %Precision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoyqinYV99u3"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.scatter(test_df['V14'],test_df['V11'],marker=\"o\", color=\"lightBlue\")\n",
        "ax.set_title('Anomalies(in red) vs Predicted Anomalies(in Green)')\n",
        "for i, txt in enumerate(test_df['V14'].index):\n",
        "       if test_y.loc[txt] == 1 :\n",
        "            ax.annotate('*', (test_df['V14'].loc[txt],test_df['V11'].loc[txt]),fontsize=13,color='Red')\n",
        "       if predictions[i] == True :\n",
        "            ax.annotate('o', (test_df['V14'].loc[txt],test_df['V11'].loc[txt]),fontsize=15,color='Green')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuCGC8oF-BNV"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(test_y, predictions)\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "name = 'ROC curve for Anomaly Detection'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8L886BA-Fji"
      },
      "source": [
        "df = pd.read_csv('credit_card_4_5.csv')\n",
        "df = df.sample(frac=1)\n",
        "df = df.drop(df.columns[0], axis=1)\n",
        "\n",
        "# amount of fraud classes 492 rows.\n",
        "fraud_df = df.loc[df['Class'] == 1][:393]\n",
        "non_fraud_df = df.loc[df['Class'] == 0][:393]\n",
        "\n",
        "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
        "\n",
        "# Shuffle dataframe rows\n",
        "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
        "# new_df.to_csv('new_data_4_5_shuffled.csv')\n",
        "new_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyiqs5nD-OfJ"
      },
      "source": [
        "y_train = new_df['Class']\n",
        "X_train = new_df.drop('Class', axis=1)\n",
        "\n",
        "dftest = pd.read_csv('credit_card_1_5.csv')\n",
        "dftest = dftest.drop(df.columns[0], axis=1)\n",
        "\n",
        "y_test = dftest['Class']\n",
        "X_test = dftest.drop('Class',axis=1)\n",
        "# Turn the values into an array for feeding the classification algorithms.\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7dSGbQo-RZZ"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# from imblearn.under_sampling import NearMiss\n",
        "# from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acc3xTSX-TxK"
      },
      "source": [
        "classifiers = {\n",
        "    \"LogisiticRegression\": LogisticRegression(),\n",
        "    \"KNearest\": KNeighborsClassifier(),\n",
        "    \"Support Vector Classifier\": SVC()\n",
        "}\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#Let's see how our model performed\n",
        "from sklearn.metrics import classification_report\n",
        "classrep = []\n",
        "confusionmatrices = {}\n",
        "predicts = {}\n",
        "for key, classifier in classifiers.items():\n",
        "    classifier.fit(X_train, y_train)\n",
        "    predictions = classifier.predict(X_test)\n",
        "    print('Classification Report of ',key,':\\n',classification_report(y_test, predictions))\n",
        "    predicts[key] = predictions\n",
        "    classrep.append(classification_report(y_test, predictions));\n",
        "    confusionmatrices[key] = confusion_matrix(y_test,predictions);\n",
        "    df_cm = pd.DataFrame(confusion_matrix(y_test,predictions), index = [i for i in ['Non-Fraudulent','Fraudulent']],\n",
        "                  columns = [i for i in ['Non-Fraudulent','Fraudulent']])\n",
        "    plt.figure(figsize = (10,7))\n",
        "    plt.title(key)\n",
        "    sn.heatmap(df_cm, annot=True)\n",
        "    name = key + 'Confusion Matrix'\n",
        "    plt.savefig(name)\n",
        "#     print('The cofusion matrix for',key,':\\n',confusion_matrix(y_test,predictions),'\\n')\n",
        "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
        "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}